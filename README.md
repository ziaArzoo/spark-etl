# ğŸŒ¤ï¸ Weather ETL Pipeline (PySpark Â· Delta Lake Â· Unity Catalog Â· Databricks)

[![Databricks Workflow](https://img.shields.io/badge/Databricks-Workflow-orange?logo=databricks)](./workflows/databricks_job_setup.md)
![PySpark](https://img.shields.io/badge/PySpark-ETL-brightgreen)
![Delta Lake](https://img.shields.io/badge/Storage-Delta%20Lake-blue)
![Unity Catalog](https://img.shields.io/badge/Governance-Unity%20Catalog-purple)
![Python](https://img.shields.io/badge/Python-3.x-yellow)
![Azure](https://img.shields.io/badge/Cloud-Azure-lightblue)

---

## ğŸ“˜ Overview

This project implements a **distributed ETL pipeline** using **PySpark**, **Delta Lake**, and **Databricks Workflows**.  
It automates ingestion, transformation, and aggregation of weather data from REST APIs, governed under **Unity Catalog**.

The pipeline runs daily on Databricks with built-in retries, alerts, and full configuration via YAML + JSON.

---

## âš™ï¸ Key Features

âœ… End-to-end ETL (Extract â†’ Transform â†’ Load)  
âœ… Delta Lake ACID storage under Unity Catalog  
âœ… Email alert on failure (to `ziaarzoo21@gmail.com`)  
âœ… Retry once after 1 minute (includes timeout retry)  
âœ… Managed Databricks Job JSON (`pipeline.json`)  
âœ… Optional Airflow DAG for hybrid orchestration  
âœ… Config-driven (YAML) + Git versioned notebooks  
âœ… **Containerized with Docker** for reproducibility and portability 

---

## ğŸ§© Architecture

```
Open-Meteo REST API
        â”‚
        â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Bronze (Raw)          â”‚ â†’  main.weather_etl.raw_weather
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Silver (Curated)      â”‚ â†’  main.weather_etl.silver_weather_curated
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Gold (Aggregated)     â”‚ â†’  main.weather_etl.gold_weather_aggregates
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§± Tech Stack

| Category | Tools |
|-----------|--------|
| Language | Python 3 Â· PySpark |
| Storage | Delta Lake |
| Governance | Unity Catalog |
| Orchestration | Databricks Jobs (new UI) |
| Cloud | Azure Databricks |
| Version Control | Git + GitHub |
| Optional | Airflow DAG, Docker |

---

## ğŸ“‚ Repository Structure

```
pyspark-etl/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Extract data
â”‚   â”œâ”€â”€ Transform Data
â”‚   â””â”€â”€ load Data
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract/
â”‚   â”œâ”€â”€ transform/
â”‚   â””â”€â”€ load/
â”œâ”€â”€ workflow/
â”‚   â”œâ”€â”€ pipeline.json
â”‚   â”œâ”€â”€ pipeline_readme.md
â”‚   â””â”€â”€ pipeline_airflow.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš¡ Databricks Workflow (New UI)

**Job name:** `Weather_ETL_Pipeline`

| Setting | Value |
|----------|--------|
| Schedule | `14 55 23 * * ?` UTC (â‰ˆ 05 : 25 IST daily) |
| Max concurrent runs | 1 |
| Retry | 1 retry Â· 1 min interval |
| Retry on timeout | âœ… |
| Queueing | âœ… Enabled |
| Performance target | `PERFORMANCE_OPTIMIZED` |
| Email alert | `ziaarzoo21@gmail.com` |

### DAG

```
extract_raw  â†’  Transform_Silver  â†’  Load_Gold
```

To import this workflow:

1. Open **Compute â†’ Jobs â†’ Create Job â†’ Import JSON**  
2. Upload `workflows/pipeline.json`  
3. Confirm notebook paths and cluster  
4. Save and run âœ…

---

## â–¶ï¸ Running the Pipeline (Manual)

Each notebook automatically loads the config:


Run in order:
1. **Extract data** â†’ creates `raw_weather`
2. **Transform Data** â†’ creates `silver_weather_curated`
3. **load Data** â†’ creates `gold_weather_aggregates`


---

## ğŸ”” Alerts & Monitoring

- Email sent on any task failure.  
- Retries once after 1 minute (backoff).  
- â€œRetry on timeoutâ€ enabled.  

---

## ğŸ§  Development Flow

1. Update code / config locally in Databricks Repos.  
2. Commit â†’ Push to GitHub.  
3. If the workflow changes, re-export JSON and update `workflows/pipeline.json`.  
4. Run job manually or on schedule.

---

## ğŸŒ€ Airflow Integration (Optional)

An equivalent Airflow DAG is provided in  
[`workflows/airflow_weather_etl_dag.py`](./workflows/pipeline_airflow.py)

Use it if you want external orchestration via Airflowâ€™s Databricks operators.

## ğŸ Author

**Zia** â€” Software developer,  
building distributed ETL systems using PySpark, Delta, and Azure Databricks.
