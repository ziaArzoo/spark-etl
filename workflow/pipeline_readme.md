# ðŸš€ Databricks Workflow â€” Weather ETL Pipeline

## ðŸ“„ Overview
The **Weather_ETL_Pipeline** orchestrates an end-to-end PySpark data pipeline using **Databricks Workflows** and **Delta Lake**.

It automates three core stages of the ETL lifecycle:

| Stage | Task Key | Notebook Path | Description |
|--------|-----------|----------------|--------------|
| ðŸŸ¤ Extract | `extract_raw` | `/Repos/ziaarzoo21@gmail.com/spark-etl/notebooks/Extract data` | Fetches weather data from REST API and loads into the Bronze (raw) Delta table. |
| âšª Transform | `Transform_Silver` | `/Repos/ziaarzoo21@gmail.com/spark-etl/notebooks/Transform Data` | Cleans, validates, and structures data into the Silver (curated) table. |
| ðŸŸ¡ Load | `Load_Gold` | `/Repos/ziaarzoo21@gmail.com/spark-etl/notebooks/load Data` | Aggregates metrics (avg temperature, humidity, precipitation) into the Gold analytics table. |

---

## âš™ï¸ Configuration Summary
| Parameter | Value |
|------------|--------|
| **Job name** | `Weather_ETL_Pipeline` |
| **Max concurrent runs** | 1 |
| **Retries per task** | 1 |
| **Retry interval** | 1 minute |
| **Retry on timeout** | âœ… Enabled |
| **Auto optimization** | âœ… Enabled (Serverless) |
| **Queueing** | âœ… Enabled |
| **Performance Target** | `PERFORMANCE_OPTIMIZED` |
| **Run as** | `ziaarzoo21@gmail.com` |

---

## ðŸ•’ Schedule
| Setting | Value |
|----------|--------|
| **Cron Expression** | `14 55 23 * * ?` |
| **Timezone** | `UTC` |
| **Status** | Active (Unpaused) |
| **Trigger Time** | 23:55:14 UTC (05:25:14 IST daily) |

---

## ðŸ“§ Notifications
| Event | Recipient |
|--------|------------|
| On failure | `ziaarzoo21@gmail.com` |

The job sends an **email alert** if any task fails.

---

## ðŸ” Retry Policy
- **1 retry per task**
- **Wait 1 minute between retries**
- **Retries on timeout:** Enabled  
- **Serverless auto-optimization:** Enabled (up to 3 additional retries)

---

## ðŸ“Š DAG Structure

Extract_Raw --> Transform_Silver --> Load_Gold

